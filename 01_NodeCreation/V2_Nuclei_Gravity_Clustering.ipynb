{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916fb959",
   "metadata": {},
   "source": [
    "# Version 2 — Population field → gravity-based city nuclei\n",
    "\n",
    "**Goal:** Start from a global population total and a spatial density field. Let density peaks “attract” nearby population into city nuclei until only cities with ≥ `min_city_pop` remain.\n",
    "\n",
    "**Method (high-level):**\n",
    "\n",
    "1. **Density field:** mixture of Gaussians + uniform baseline + low-frequency noise.\n",
    "2. **Allocate people:** multinomial draw over grid cells ⇒ exact `total_population`.\n",
    "3. **Detect peaks:** local maxima above a percentile, with non-maximum suppression (min separation).\n",
    "4. **Gravity assignment:** each cell’s people go to peak `j` with attractiveness\n",
    "   `A_j = w_j / (dist + eps)^γ`, where `w_j` scales with the peak’s density.\n",
    "5. **Prune & reassign:** iteratively remove cities with pop `< min_city_pop`, reassign their cells.\n",
    "6. **Enforce city count:** if after pruning we have `< n_cities`, relax thresholds and/or split the largest city into two viable parts (both ≥ `min_city_pop`) and reassign, until `n_cities` is met or no further split is possible.\n",
    "7. **City center:** population-weighted centroid of its assigned cells.\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* `nodes.csv` → `id, x_km, y_km, pop, n_cells_assigned, radius_km`\n",
    "* `meta.json` → full config, metrics, generator info, hashes\n",
    "* `preview.png` → scatter (size/color = population, top-3 annotated)\n",
    "* `population_heatmap.png` → heatmap of allocated people per grid cell\n",
    "\n",
    "---\n",
    "\n",
    "## Key parameters (in `V2Config`)\n",
    "\n",
    "* **Global/population**\n",
    "\n",
    "  * `total_population` (e.g., `5_000_000`)\n",
    "  * `seed`\n",
    "* **Region/discretization**\n",
    "\n",
    "  * `bbox_km = (minx, miny, maxx, maxy)`\n",
    "  * `grid_res_km` (cell size; smaller ⇒ more cells/finer detail)\n",
    "* **Density field**\n",
    "\n",
    "  * `n_centers` (mixture components)\n",
    "  * `center_sigma_km_min`, `center_sigma_km_max` (spread per center)\n",
    "  * `baseline_frac` (uniform floor)\n",
    "  * `noise_amp`, `noise_grid` (low-freq variability)\n",
    "* **Peak detection**\n",
    "\n",
    "  * `peaks_percentile` (initial threshold)\n",
    "  * `min_peak_separation_km`\n",
    "* **Gravity assignment**\n",
    "\n",
    "  * `gamma` (distance exponent), `eps_km` (softening)\n",
    "* **City pruning / constraints**\n",
    "\n",
    "  * `min_city_pop` (e.g., `1_000`)\n",
    "  * `n_cities` (minimum number of cities to produce)\n",
    "  * `peaks_percentile_floor`, `separation_shrink`, `max_relax_iters` (how we relax detection to grow city count)\n",
    "\n",
    "---\n",
    "\n",
    "## Validation\n",
    "\n",
    "* **Population conservation:** `sum(nodes.pop) == total_population`.\n",
    "* **Threshold compliance:** `min(nodes.pop) ≥ min_city_pop`.\n",
    "* **City count:** `len(nodes) ≥ n_cities` (otherwise raise with guidance).\n",
    "* **Sanity metrics (stored in meta):** pop percentiles, number of peaks used, relaxation summary, grid info.\n",
    "\n",
    "---\n",
    "\n",
    "## Tuning tips\n",
    "\n",
    "* Too few cities? Try **lower** `min_city_pop`, **finer** `grid_res_km`, **lower** `peaks_percentile`, **smaller** `min_peak_separation_km`, or **increase** `n_centers`/reduce `center_sigma`.\n",
    "* Cities too fragmented? **Increase** `min_peak_separation_km` or **raise** `peaks_percentile`.\n",
    "* Over-dominant mega-city? **Increase** `gamma` (stronger distance penalty) or **reduce** center sigmas near that core.\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "* Run the V2 notebook cell; adjust `V2Config` (especially `total_population`, `grid_res_km`, `min_city_pop`, `n_cities`).\n",
    "* Artifacts are written to `out_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576884ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Nodes V2] Build complete:\n",
      "----------------------------------------\n",
      "Cities: 20 | Total pop: 5,000,000\n",
      "Saved: nodes → maps/sv1.2/dv0.1_v2_density_cities\\nodes.csv\n",
      "       preview → maps/sv1.2/dv0.1_v2_density_cities\\preview.png\n",
      "       meta → maps/sv1.2/dv0.1_v2_density_cities\\meta.json\n",
      "Metrics hash: ab376ddd767689b9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter notebook cell — Version 2 (Nodes only)\n",
    "Population density → city nuclei via gravity-like clustering\n",
    "\n",
    "Goal: Start from a total population spread over space with higher/lower density areas,\n",
    "then let density peaks attract nearby population into city nuclei until only cities with\n",
    "≥ min_city_pop remain (e.g., 1,000 inhabitants).\n",
    "\n",
    "- Field: Mixture-of-Gaussians density + low‑frequency noise + uniform baseline\n",
    "- Allocation: Multinomial over grid cells (so total people is exact)\n",
    "- Peaks: Local maxima of the density with non‑maximum suppression (min separation)\n",
    "- Assignment: Each cell’s population goes to the peak with max attractiveness\n",
    "             A_j = weight_j / (distance + eps)^gamma, weight_j ∝ peak density\n",
    "- Pruning: Iteratively remove cities with pop < min_city_pop and reassign their cells\n",
    "- City center: Population‑weighted centroid of assigned cells\n",
    "- Outputs: nodes.csv, meta.json, preview.png (color = population; top‑3 annotated)\n",
    "\n",
    "Usage: run this cell. Edit `V2Config` at the end.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class V2Config:\n",
    "    seed: int = 42\n",
    "    total_population: int = 5_000_000\n",
    "    bbox_km: Tuple[float, float, float, float] = (0.0, 0.0, 200.0, 200.0)  # (minx, miny, maxx, maxy)\n",
    "\n",
    "    # Grid\n",
    "    grid_res_km: float = 2.0  # cell size (km). 200 km / 2 km ⇒ 100×100 grid\n",
    "\n",
    "    # Density mixture (centers auto if None)\n",
    "    n_centers: int = 4\n",
    "    center_sigma_km_min: float = 12.0\n",
    "    center_sigma_km_max: float = 35.0\n",
    "    baseline_frac: float = 0.05   # baseline level as a fraction of mean Gaussian field\n",
    "\n",
    "    # Low‑frequency noise (coarse grid upsampled bilinearly)\n",
    "    noise_amp: float = 0.30       # multiplicative amplitude; 0.3 ⇒ ×(1±0.3)\n",
    "    noise_grid: Tuple[int, int] = (10, 10)  # (rows, cols) of coarse noise grid\n",
    "\n",
    "    # Peak detection (on density field)\n",
    "    peaks_percentile: float = 92.0  # keep local maxima above this percentile\n",
    "    min_peak_separation_km: float = 8.0\n",
    "\n",
    "    # Gravity assignment\n",
    "    gamma: float = 1.7             # distance exponent in attractiveness\n",
    "    eps_km: float = 0.5            # small distance softening (km)\n",
    "\n",
    "    # City pruning\n",
    "    min_city_pop: int = 1_000\n",
    "\n",
    "    n_cities: int = 8                    # target minimum number of cities\n",
    "    peaks_percentile_floor: float = 60.0 # how far we can relax the threshold\n",
    "    separation_shrink: float = 0.85      # shrink factor for min peak separation per relax step\n",
    "    max_relax_iters: int = 10            # max relax attempts to reach n_cities\n",
    "\n",
    "\n",
    "    # Output & metadata\n",
    "    out_dir: str = \"maps/sv1.2/dv0.1_v2_density_cities\"\n",
    "    crs: str = \"EPSG:3857\"\n",
    "    schema_version: str = \"1.2\"  # optional new cols (`radius_km`, `n_cells_assigned`)\n",
    "    dataset_version: str = \"0.1\"\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def _bbox_arrays(cfg: V2Config):\n",
    "    minx, miny, maxx, maxy = cfg.bbox_km\n",
    "    W, H = maxx - minx, maxy - miny\n",
    "    nx = int(np.ceil(W / cfg.grid_res_km))\n",
    "    ny = int(np.ceil(H / cfg.grid_res_km))\n",
    "    x = minx + (np.arange(nx) + 0.5) * cfg.grid_res_km\n",
    "    y = miny + (np.arange(ny) + 0.5) * cfg.grid_res_km\n",
    "    X, Y = np.meshgrid(x, y)  # shape (ny, nx)\n",
    "    return X, Y, x, y, nx, ny\n",
    "\n",
    "\n",
    "def _dirichlet_weights(k: int) -> np.ndarray:\n",
    "    w = np.random.rand(k)\n",
    "    w = w + 0.01  # avoid zeros\n",
    "    return w / w.sum()\n",
    "\n",
    "\n",
    "def _upsample_bilinear(coarse: np.ndarray, ny: int, nx: int) -> np.ndarray:\n",
    "    \"\"\"Simple bilinear upsample using two 1D interpolations (no SciPy).\"\"\"\n",
    "    cy, cx = coarse.shape\n",
    "    x_old = np.linspace(0.0, 1.0, cx)\n",
    "    x_new = np.linspace(0.0, 1.0, nx)\n",
    "    # interp along x for each row\n",
    "    tmp = np.array([np.interp(x_new, x_old, coarse[i, :]) for i in range(cy)])  # (cy, nx)\n",
    "    y_old = np.linspace(0.0, 1.0, cy)\n",
    "    y_new = np.linspace(0.0, 1.0, ny)\n",
    "    # interp along y for each column\n",
    "    out = np.array([np.interp(y_new, y_old, tmp[:, j]) for j in range(nx)]).T  # (ny, nx)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_density_field(cfg: V2Config) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    X, Y, x, y, nx, ny = _bbox_arrays(cfg)\n",
    "\n",
    "    # Mixture of Gaussians\n",
    "    centers = np.column_stack([\n",
    "        np.random.uniform(x.min(), x.max(), size=cfg.n_centers),\n",
    "        np.random.uniform(y.min(), y.max(), size=cfg.n_centers),\n",
    "    ])\n",
    "    sigmas = np.random.uniform(cfg.center_sigma_km_min, cfg.center_sigma_km_max, size=cfg.n_centers)\n",
    "    weights = _dirichlet_weights(cfg.n_centers)\n",
    "\n",
    "    G = np.zeros((ny, nx), dtype=float)\n",
    "    for (cx, cy), s, w in zip(centers, sigmas, weights):\n",
    "        G += w * np.exp(-((X - cx) ** 2 + (Y - cy) ** 2) / (2 * s * s))\n",
    "\n",
    "    # Baseline\n",
    "    baseline = cfg.baseline_frac * (G.mean() + 1e-9)\n",
    "\n",
    "    # Low-frequency noise\n",
    "    if cfg.noise_amp > 0:\n",
    "        ngy, ngx = cfg.noise_grid\n",
    "        coarse = np.random.rand(ngy, ngx)\n",
    "        noise = _upsample_bilinear(coarse, ny, nx)\n",
    "        noise = (noise - 0.5) * 2.0  # ~[-1,1]\n",
    "        field = (G + baseline) * (1.0 + cfg.noise_amp * noise)\n",
    "        field = np.clip(field, a_min=baseline * 0.1, a_max=None)\n",
    "    else:\n",
    "        field = G + baseline\n",
    "\n",
    "    info = {\n",
    "        \"centers\": centers.tolist(),\n",
    "        \"sigmas\": sigmas.tolist(),\n",
    "        \"weights\": weights.tolist(),\n",
    "        \"baseline\": baseline,\n",
    "        \"noise_amp\": cfg.noise_amp,\n",
    "        \"noise_grid\": cfg.noise_grid,\n",
    "    }\n",
    "    return field, info\n",
    "\n",
    "\n",
    "def _find_local_maxima(field: np.ndarray, percentile: float, min_sep_cells: int) -> List[Tuple[int, int, float]]:\n",
    "    ny, nx = field.shape\n",
    "    thr = np.percentile(field, percentile)\n",
    "    peaks: List[Tuple[int, int, float]] = []\n",
    "    for i in range(1, ny - 1):\n",
    "        for j in range(1, nx - 1):\n",
    "            v = field[i, j]\n",
    "            if v < thr:\n",
    "                continue\n",
    "            nb = field[i-1:i+2, j-1:j+2]\n",
    "            if v >= nb.max():\n",
    "                peaks.append((i, j, float(v)))\n",
    "    # Non-maximum suppression by min_sep_cells (greedy)\n",
    "    peaks.sort(key=lambda t: t[2], reverse=True)\n",
    "    accepted: List[Tuple[int, int, float]] = []\n",
    "    for i, j, v in peaks:\n",
    "        ok = True\n",
    "        for ia, ja, _ in accepted:\n",
    "            if (i - ia) ** 2 + (j - ja) ** 2 < (min_sep_cells ** 2):\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            accepted.append((i, j, v))\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def _assign_cells_to_peaks(counts: np.ndarray, field: np.ndarray, peaks: List[Tuple[int, int, float]], cfg: V2Config) -> np.ndarray:\n",
    "    \"\"\"Return array of shape (ny, nx) with city index per cell (−1 if no peaks).\"\"\"\n",
    "    ny, nx = counts.shape\n",
    "    if not peaks:\n",
    "        return -np.ones((ny, nx), dtype=int)\n",
    "\n",
    "    # Precompute peak weights and coordinates in km\n",
    "    X, Y, x, y, nx2, ny2 = _bbox_arrays(cfg)\n",
    "    assert nx2 == nx and ny2 == ny\n",
    "\n",
    "    px = np.array([x[int(j)] for (_, j, _) in peaks])  # careful: peaks store (row=i, col=j)\n",
    "    py = np.array([y[int(i)] for (i, _, _) in peaks])\n",
    "\n",
    "    peak_weight = np.array([v for (_, _, v) in peaks], dtype=float)\n",
    "    peak_weight = peak_weight / (peak_weight.max() + 1e-12)\n",
    "\n",
    "    # We compute attractiveness for each peak: w / (dist + eps)^gamma\n",
    "    eps2 = (cfg.eps_km ** 2)\n",
    "\n",
    "    # Flatten coordinates for vectorization\n",
    "    XX = X.reshape(-1)\n",
    "    YY = Y.reshape(-1)\n",
    "    counts_flat = counts.reshape(-1)\n",
    "\n",
    "    # Only consider cells with people\n",
    "    active_idx = np.where(counts_flat > 0)[0]\n",
    "    XXa = XX[active_idx][:, None]\n",
    "    YYa = YY[active_idx][:, None]\n",
    "\n",
    "    # distances to peaks (active cells × n_peaks)\n",
    "    dx = XXa - px[None, :]\n",
    "    dy = YYa - py[None, :]\n",
    "    dist2 = dx * dx + dy * dy\n",
    "\n",
    "    attractiveness = peak_weight[None, :] / np.power(dist2 + eps2, cfg.gamma / 2.0)\n",
    "    best = np.argmax(attractiveness, axis=1)\n",
    "\n",
    "    assign = -np.ones(XX.shape[0], dtype=int)\n",
    "    assign[active_idx] = best\n",
    "    return assign.reshape(ny, nx)\n",
    "\n",
    "\n",
    "def _city_stats_from_assignment(assign: np.ndarray, counts: np.ndarray, cfg: V2Config) -> Tuple[pd.DataFrame, Dict[int, np.ndarray]]:\n",
    "    \"\"\"Compute city populations, centroids, radius, and keep cell masks per city.\"\"\"\n",
    "    X, Y, *_ = _bbox_arrays(cfg)\n",
    "    ny, nx = counts.shape\n",
    "\n",
    "    city_ids = np.unique(assign[assign >= 0])\n",
    "    masks: Dict[int, np.ndarray] = {}\n",
    "    rows = []\n",
    "    for cid in city_ids:\n",
    "        mask = assign == cid\n",
    "        pop = int(counts[mask].sum())\n",
    "        if pop <= 0:\n",
    "            continue\n",
    "        masks[cid] = mask\n",
    "        # Pop-weighted centroid\n",
    "        w = counts[mask].astype(float)\n",
    "        xs = X[mask]\n",
    "        ys = Y[mask]\n",
    "        x_c = float((w * xs).sum() / w.sum())\n",
    "        y_c = float((w * ys).sum() / w.sum())\n",
    "        n_cells = int(mask.sum())\n",
    "        area_km2 = n_cells * (cfg.grid_res_km ** 2)\n",
    "        radius_km = float(np.sqrt(area_km2 / np.pi))\n",
    "        rows.append({\"city_id\": int(cid), \"x_km\": x_c, \"y_km\": y_c, \"pop\": pop, \"n_cells_assigned\": n_cells, \"radius_km\": radius_km})\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"pop\", ascending=False).reset_index(drop=True)\n",
    "    return df, masks\n",
    "\n",
    "\n",
    "def _prune_and_reassign(assign: np.ndarray, counts: np.ndarray, peaks: List[Tuple[int, int, float]], cfg: V2Config) -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Iteratively remove cities below threshold and reassign their cells to survivors.\"\"\"\n",
    "    ny, nx = counts.shape\n",
    "    while True:\n",
    "        df, masks = _city_stats_from_assignment(assign, counts, cfg)\n",
    "        if df.empty:\n",
    "            raise RuntimeError(\"No cities formed — check parameters.\")\n",
    "        low = df[df[\"pop\"] < cfg.min_city_pop]\n",
    "        if low.empty or len(df) == 1:\n",
    "            # Done\n",
    "            survivors = df[\"city_id\"].tolist()\n",
    "            return assign, survivors\n",
    "        # Remove the smallest city under threshold\n",
    "        remove_id = int(low.sort_values(\"pop\").iloc[0][\"city_id\"])\n",
    "        # Reassign its cells to the best among survivors\n",
    "        survivors = [int(cid) for cid in df[\"city_id\"].tolist() if cid != remove_id]\n",
    "\n",
    "        # Build survivors peak subset\n",
    "        surv_peaks = [peaks[cid] for cid in survivors]\n",
    "        # Temporarily set these cells to -1 to be reassigned\n",
    "        rem_mask = masks[remove_id]\n",
    "        assign[rem_mask] = -1\n",
    "\n",
    "        # Reassign only the removed cells by recomputing best survivor for those cells\n",
    "        # Compute attractiveness for survivors\n",
    "        X, Y, x, y, nx2, ny2 = _bbox_arrays(cfg)\n",
    "        px = np.array([x[int(j)] for (i, j, v) in surv_peaks])\n",
    "        py = np.array([y[int(i)] for (i, j, v) in surv_peaks])\n",
    "        peak_weight = np.array([v for (i, j, v) in surv_peaks], dtype=float)\n",
    "        peak_weight = peak_weight / (peak_weight.max() + 1e-12)\n",
    "        eps2 = (cfg.eps_km ** 2)\n",
    "\n",
    "        idx_cells = np.where(rem_mask.reshape(-1))[0]\n",
    "        XX = X.reshape(-1)[idx_cells][:, None]\n",
    "        YY = Y.reshape(-1)[idx_cells][:, None]\n",
    "        dx = XX - px[None, :]\n",
    "        dy = YY - py[None, :]\n",
    "        dist2 = dx * dx + dy * dy\n",
    "        attractiveness = peak_weight[None, :] / np.power(dist2 + eps2, cfg.gamma / 2.0)\n",
    "        best = np.argmax(attractiveness, axis=1)\n",
    "        reassigned = np.array([survivors[b] for b in best], dtype=int)\n",
    "\n",
    "        # Write back\n",
    "        flat_assign = assign.reshape(-1)\n",
    "        flat_assign[idx_cells] = reassigned\n",
    "        assign = flat_assign.reshape(ny, nx)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main generator / validator / saver\n",
    "# ------------------------------\n",
    "\n",
    "def generate_nodes_v2(cfg: V2Config) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Guarantee >= cfg.n_cities with each city >= cfg.min_city_pop by:\n",
    "      1) detecting density peaks (relaxing thresholds if needed),\n",
    "      2) assigning by gravity,\n",
    "      3) pruning tiny cities,\n",
    "      4) if still short, SPLITTING the largest city into two (by weighted median along the widest axis)\n",
    "         and repeating assignment until the target count is reached or no city can be split further.\n",
    "    \"\"\"\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    # ----- Density + allocation -----\n",
    "    field, field_info = generate_density_field(cfg)\n",
    "    probs = field / field.sum()\n",
    "    X, Y, x, y, nx, ny = _bbox_arrays(cfg)\n",
    "    counts = np.random.multinomial(cfg.total_population, probs.reshape(-1)).reshape(ny, nx)\n",
    "\n",
    "    def find_peaks(percentile: float, sep_cells: int) -> List[Tuple[int, int, float]]:\n",
    "        return _find_local_maxima(field, percentile, max(1, sep_cells))\n",
    "\n",
    "    def assign_and_stats(peaks: List[Tuple[int, int, float]]):\n",
    "        assign = _assign_cells_to_peaks(counts, field, peaks, cfg)\n",
    "        df_cities, masks = _city_stats_from_assignment(assign, counts, cfg)\n",
    "        return assign, df_cities, masks\n",
    "\n",
    "    def prune_below_threshold(assign: np.ndarray, peaks: List[Tuple[int, int, float]]):\n",
    "        # use existing prune (merges tiny cities into survivors)\n",
    "        assign2, survivors = _prune_and_reassign(assign.copy(), counts, peaks, cfg)\n",
    "        df2, masks2 = _city_stats_from_assignment(assign2, counts, cfg)\n",
    "        return assign2, df2, masks2\n",
    "\n",
    "    def split_city_mask(mask: np.ndarray) -> List[Tuple[int, int, float]] | None:\n",
    "        \"\"\"Split one city into two by weighted median along widest axis; return two new peak tuples (i,j,value).\"\"\"\n",
    "        w = counts[mask].astype(float)\n",
    "        if w.sum() < 2 * cfg.min_city_pop or mask.sum() < 2:\n",
    "            return None\n",
    "        xs = X[mask]; ys = Y[mask]\n",
    "        # choose axis with larger variance\n",
    "        varx, vary = np.var(xs, ddof=0), np.var(ys, ddof=0)\n",
    "        coord = xs if varx >= vary else ys\n",
    "        order = np.argsort(coord)\n",
    "        w_sorted = w[order]\n",
    "        xs_sorted, ys_sorted = xs[order], ys[order]\n",
    "        csum = np.cumsum(w_sorted)\n",
    "        # cut near half but ensure both sides >= min_city_pop\n",
    "        total = csum[-1]\n",
    "        cut_idx = np.searchsorted(csum, total / 2.0)\n",
    "        # expand cut to meet threshold\n",
    "        left_ok = lambda k: csum[k] >= cfg.min_city_pop\n",
    "        right_ok = lambda k: (total - csum[k]) >= cfg.min_city_pop\n",
    "        k = int(np.clip(cut_idx, 1, len(w_sorted) - 2))\n",
    "        moved = True\n",
    "        while moved:\n",
    "            moved = False\n",
    "            if not left_ok(k):\n",
    "                k += 1; moved = True\n",
    "            if not right_ok(k):\n",
    "                k -= 1; moved = True\n",
    "            if k <= 0 or k >= len(w_sorted) - 1:\n",
    "                return None  # cannot split with thresholds\n",
    "        # weighted centroids for two parts\n",
    "        wL, wR = w_sorted[:k], w_sorted[k:]\n",
    "        xL = float(np.average(xs_sorted[:k], weights=wL))\n",
    "        yL = float(np.average(ys_sorted[:k], weights=wL))\n",
    "        xR = float(np.average(xs_sorted[k:],  weights=wR))\n",
    "        yR = float(np.average(ys_sorted[k:],  weights=wR))\n",
    "        # snap to nearest grid cells\n",
    "        jL, iL = int(np.argmin(np.abs(x - xL))), int(np.argmin(np.abs(y - yL)))\n",
    "        jR, iR = int(np.argmin(np.abs(x - xR))), int(np.argmin(np.abs(y - yR)))\n",
    "        pL = (iL, jL, float(field[iL, jL]))\n",
    "        pR = (iR, jR, float(field[iR, jR]))\n",
    "        return [pL, pR]\n",
    "\n",
    "    # ---- 1) initial peaks with relaxation ----\n",
    "    current_pct = float(getattr(cfg, \"peaks_percentile\", 92.0))\n",
    "    current_sep = int(round(cfg.min_peak_separation_km / cfg.grid_res_km))\n",
    "    pct_floor = float(getattr(cfg, \"peaks_percentile_floor\", 60.0))\n",
    "    shrink = float(getattr(cfg, \"separation_shrink\", 0.85))\n",
    "    max_relax = int(getattr(cfg, \"max_relax_iters\", 10))\n",
    "\n",
    "    peaks = find_peaks(current_pct, current_sep)\n",
    "    for _ in range(max_relax):\n",
    "        assign, df, masks = assign_and_stats(peaks)\n",
    "        # prune tiny cities (merge) only if we still have >= n_cities afterwards\n",
    "        assign_p, df_p, masks_p = prune_below_threshold(assign, peaks)\n",
    "        df = df_p; masks = masks_p; assign = assign_p\n",
    "        if len(df) >= cfg.n_cities:\n",
    "            break\n",
    "        # relax detection if we can\n",
    "        new_pct = max(pct_floor, current_pct - 5.0)\n",
    "        new_sep = max(1, int(np.ceil(current_sep * shrink)))\n",
    "        new_peaks = find_peaks(new_pct, new_sep)\n",
    "        if len(new_peaks) > len(peaks):  # only accept if we actually got more\n",
    "            peaks, current_pct, current_sep = new_peaks, new_pct, new_sep\n",
    "        else:\n",
    "            break  # no further improvement\n",
    "\n",
    "    # ---- 2) enforce minimum by splitting largest cities if needed ----\n",
    "    # always recompute with current peaks\n",
    "    assign, df, masks = assign_and_stats(peaks)\n",
    "    # prune tiny ones first (merge them up)\n",
    "    assign, df, masks = prune_below_threshold(assign, peaks)\n",
    "\n",
    "    # if still short, repeatedly split the largest splittable city\n",
    "    attempts = 0\n",
    "    while len(df) < cfg.n_cities:\n",
    "        attempts += 1\n",
    "        if attempts > 200:  # safety\n",
    "            break\n",
    "        # pick largest city that can be split\n",
    "        df_sorted = df.sort_values(\"pop\", ascending=False)\n",
    "        split_done = False\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            cid = int(row[\"city_id\"])\n",
    "            mask = masks[cid]\n",
    "            new_two = split_city_mask(mask)\n",
    "            if new_two is None:\n",
    "                continue\n",
    "            # replace this city's peak with two new peaks\n",
    "            peaks = [p for idx, p in enumerate(peaks) if idx != cid] + new_two\n",
    "            # reassign & prune\n",
    "            assign, df, masks = assign_and_stats(peaks)\n",
    "            assign, df, masks = prune_below_threshold(assign, peaks)\n",
    "            split_done = True\n",
    "            if len(df) >= cfg.n_cities:\n",
    "                break\n",
    "        if not split_done:\n",
    "            break  # no city can be split further while respecting min_city_pop\n",
    "\n",
    "    if len(df) < cfg.n_cities:\n",
    "        raise RuntimeError(f\"Could not reach the target number of cities (got {len(df)}, want {cfg.n_cities}). \"\n",
    "                           f\"Try decreasing min_city_pop, increasing total_population, or using finer grid_res_km.\")\n",
    "\n",
    "    # ---- finalize nodes ----\n",
    "    df_cities = df\n",
    "    assert int(df_cities[\"pop\"].sum()) == int(cfg.total_population)\n",
    "\n",
    "    df_nodes = df_cities.rename(columns={\"city_id\": \"id\"})[\n",
    "        [\"id\", \"x_km\", \"y_km\", \"pop\", \"n_cells_assigned\", \"radius_km\"]\n",
    "    ].copy()\n",
    "    df_nodes[\"id\"] = df_nodes[\"id\"].astype(int)\n",
    "\n",
    "    extras = {\n",
    "        \"grid\": {\"nx\": nx, \"ny\": ny, \"res_km\": cfg.grid_res_km},\n",
    "        \"peaks_count_used\": int(len(peaks)),\n",
    "        \"relaxation\": {\n",
    "            \"final_percentile\": current_pct,\n",
    "            \"final_sep_cells\": current_sep,\n",
    "            \"target_n_cities\": int(cfg.n_cities),\n",
    "            \"achieved_n_cities\": int(len(df_nodes)),\n",
    "        },\n",
    "        \"field_info\": field_info,\n",
    "        \"counts\": counts,\n",
    "        \"field\": field,\n",
    "    }\n",
    "    return df_nodes, extras\n",
    "\n",
    "def validate_nodes(df_nodes: pd.DataFrame, cfg: V2Config) -> Dict[str, Any]:\n",
    "    metrics: Dict[str, Any] = {}\n",
    "    n = len(df_nodes)\n",
    "    if n == 0:\n",
    "        raise AssertionError(\"No cities produced\")\n",
    "    if (df_nodes[\"pop\"] < cfg.min_city_pop).any():\n",
    "        raise AssertionError(\"Found a city below min_city_pop after pruning\")\n",
    "    metrics[\"n_cities\"] = int(n)\n",
    "    metrics[\"total_population\"] = int(df_nodes[\"pop\"].sum())\n",
    "    metrics[\"pop_percentiles\"] = {q: int(np.percentile(df_nodes[\"pop\"], q)) for q in (5, 25, 50, 75, 90, 95, 99)}\n",
    "    return metrics\n",
    "\n",
    "def preview_nodes(df_nodes: pd.DataFrame, cfg: V2Config, save_path: str) -> None:\n",
    "    minx, miny, maxx, maxy = cfg.bbox_km\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    vmax = df_nodes[\"pop\"].max()\n",
    "    sc = plt.scatter(\n",
    "        df_nodes[\"x_km\"], df_nodes[\"y_km\"],\n",
    "        s=10 + 90 * np.sqrt(df_nodes[\"pop\"].values / vmax),\n",
    "        c=df_nodes[\"pop\"].values.astype(float),\n",
    "    )\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label(\"Population\")\n",
    "    try:\n",
    "        cbar.ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Annotate top‑3 by population\n",
    "    top3 = df_nodes.nlargest(3, \"pop\").copy()\n",
    "    dx = 0.01 * (maxx - minx)\n",
    "    dy = 0.01 * (maxy - miny)\n",
    "    for _, row in top3.iterrows():\n",
    "        label = f\"{int(row['pop']):,}\"\n",
    "        plt.text(\n",
    "            row[\"x_km\"] + dx,\n",
    "            row[\"y_km\"] + dy,\n",
    "            label,\n",
    "            fontsize=8,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7),\n",
    "        )\n",
    "\n",
    "    plt.title(\"Nodes — V2 (density → gravity cities)\")\n",
    "    plt.xlabel(\"x (km)\")\n",
    "    plt.ylabel(\"y (km)\")\n",
    "    plt.xlim(minx, maxx)\n",
    "    plt.ylim(miny, maxy)\n",
    "    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def compute_metrics_hash(metrics: Dict[str, Any]) -> str:\n",
    "    blob = json.dumps(metrics, sort_keys=True).encode(\"utf-8\")\n",
    "    return hashlib.sha256(blob).hexdigest()[:16]\n",
    "\n",
    "def save_artifacts(df_nodes: pd.DataFrame, cfg: V2Config, metrics: Dict[str, Any], extras: Dict[str, Any]) -> Dict[str, str]:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    nodes_path = os.path.join(cfg.out_dir, \"nodes.csv\")\n",
    "    preview_path = os.path.join(cfg.out_dir, \"preview.png\")\n",
    "    heatmap_path = os.path.join(cfg.out_dir, \"population_heatmap.png\")\n",
    "    meta_path = os.path.join(cfg.out_dir, \"meta.json\")\n",
    "\n",
    "    # Save nodes CSV\n",
    "    df_nodes.to_csv(nodes_path, index=False)\n",
    "\n",
    "    # Existing scatter preview of cities\n",
    "    preview_nodes(df_nodes, cfg, preview_path)\n",
    "\n",
    "    # --- NEW: heatmap of individual distribution (grid cell counts) ---\n",
    "    counts = extras.get(\"counts\", None)  # expected shape (ny, nx)\n",
    "    if counts is not None:\n",
    "        minx, miny, maxx, maxy = cfg.bbox_km\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # imshow with spatial extent to align axes with km coordinates\n",
    "        plt.imshow(\n",
    "            counts,\n",
    "            origin=\"lower\",\n",
    "            extent=[minx, maxx, miny, maxy],\n",
    "            aspect=\"equal\",\n",
    "        )\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label(\"People per cell\")\n",
    "        try:\n",
    "            cbar.ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "        except Exception:\n",
    "            pass\n",
    "        plt.title(\"Population distribution (heatmap)\")\n",
    "        plt.xlabel(\"x (km)\")\n",
    "        plt.ylabel(\"y (km)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(heatmap_path, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # Meta\n",
    "    meta = {\n",
    "        \"schema_version\": cfg.schema_version,\n",
    "        \"dataset_version\": cfg.dataset_version,\n",
    "        \"crs\": cfg.crs,\n",
    "        \"seed\": cfg.seed,\n",
    "        \"generator\": {\n",
    "            \"name\": \"nodes_v2_density_to_cities\",\n",
    "            \"params\": {\n",
    "                \"total_population\": cfg.total_population,\n",
    "                \"bbox_km\": cfg.bbox_km,\n",
    "                \"grid_res_km\": cfg.grid_res_km,\n",
    "                \"n_centers\": cfg.n_centers,\n",
    "                \"center_sigma_km_min\": cfg.center_sigma_km_min,\n",
    "                \"center_sigma_km_max\": cfg.center_sigma_km_max,\n",
    "                \"baseline_frac\": cfg.baseline_frac,\n",
    "                \"noise_amp\": cfg.noise_amp,\n",
    "                \"noise_grid\": cfg.noise_grid,\n",
    "                \"peaks_percentile\": cfg.peaks_percentile,\n",
    "                \"min_peak_separation_km\": cfg.min_peak_separation_km,\n",
    "                \"gamma\": cfg.gamma,\n",
    "                \"eps_km\": cfg.eps_km,\n",
    "                \"min_city_pop\": cfg.min_city_pop,\n",
    "            },\n",
    "        },\n",
    "        \"extras_summary\": {\n",
    "            \"grid\": extras.get(\"grid\"),\n",
    "            \"peaks_count\": extras.get(\"peaks_count\"),\n",
    "        },\n",
    "        \"artifacts\": {\n",
    "            \"nodes_csv\": nodes_path,\n",
    "            \"preview_png\": preview_path,\n",
    "            \"population_heatmap_png\": heatmap_path if counts is not None else None,\n",
    "        },\n",
    "        \"metrics\": metrics,\n",
    "        \"metrics_hash\": compute_metrics_hash(metrics),\n",
    "        \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\"nodes\": nodes_path, \"preview\": preview_path, \"heatmap\": (heatmap_path if counts is not None else None), \"meta\": meta_path}\n",
    "\n",
    "# ------------------------------\n",
    "# Orchestration\n",
    "# ------------------------------\n",
    "\n",
    "def main(cfg: V2Config | None = None) -> pd.DataFrame:\n",
    "    cfg = cfg or V2Config()\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    df_nodes, extras = generate_nodes_v2(cfg)\n",
    "    metrics = validate_nodes(df_nodes, cfg)\n",
    "    paths = save_artifacts(df_nodes, cfg, metrics, extras)\n",
    "\n",
    "    print(\"\\n[Nodes V2] Build complete:\\n\" + \"-\" * 40)\n",
    "    print(f\"Cities: {len(df_nodes)} | Total pop: {metrics['total_population']:,}\")\n",
    "    print(f\"Saved: nodes → {paths['nodes']}\\n       preview → {paths['preview']}\\n       meta → {paths['meta']}\")\n",
    "    print(f\"Metrics hash: {compute_metrics_hash(metrics)}\")\n",
    "    return df_nodes\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "_cfg = V2Config(\n",
    "    seed=42,\n",
    "    total_population=5_000_000,\n",
    "    bbox_km=(0.0, 0.0, 200.0, 200.0),\n",
    "    grid_res_km=2.0,\n",
    "    n_centers=4,\n",
    "    center_sigma_km_min=12.0,\n",
    "    center_sigma_km_max=35.0,\n",
    "    baseline_frac=0.05,\n",
    "    noise_amp=0.30,\n",
    "    noise_grid=(10, 10),\n",
    "    peaks_percentile=92.0,\n",
    "    min_peak_separation_km=8.0,\n",
    "    gamma=1.7,\n",
    "    eps_km=0.5,\n",
    "    min_city_pop=1_000,\n",
    "    n_cities=20,\n",
    "    peaks_percentile_floor=60.0,\n",
    "    separation_shrink=0.85,\n",
    "    max_relax_iters=10,\n",
    "    out_dir=\"maps/sv1.2/dv0.1_v2_density_cities\",\n",
    ")\n",
    "\n",
    "_ = main(_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
