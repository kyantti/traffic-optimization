{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0312b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6228513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found: 10\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds01_seed42_tp3M_nc15\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds01_seed42_tp3M_nc15\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds02_seed43_tp3.5M_nc18\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds02_seed43_tp3.5M_nc18\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds03_seed44_tp4M_nc20\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds03_seed44_tp4M_nc20\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds04_seed45_tp2.5M_nc22\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds04_seed45_tp2.5M_nc22\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds05_seed46_tp5M_nc24\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds05_seed46_tp5M_nc24\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds06_seed47_tp5.5M_nc26\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds06_seed47_tp5.5M_nc26\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds07_seed48_tp1M_nc28\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds07_seed48_tp1M_nc28\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds08_seed49_tp6.5M_nc30\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds08_seed49_tp6.5M_nc30\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds09_seed50_tp0.5M_nc32\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds09_seed50_tp0.5M_nc32\\solutions\\v1_greedy_mst\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds10_seed51_tp1.5M_nc35\\solutions\\v0_random_tree_seed42\n",
      "[OK] sim_v0 created → sv1.2\\dv0.1_ds10_seed51_tp1.5M_nc35\\solutions\\v1_greedy_mst\n",
      "\n",
      "[Transport Sim V0] Done. New simulations: 20 | Skipped (already existed): 0\n",
      "Global overview: C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\\transport_sim_v0_overview.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter notebook cell — Transport Simulation V0 (length-only network)\n",
    "\n",
    "Batch-aware runner:\n",
    "- Scans every dataset under `dataset_root` (folders that contain a top-level `nodes.csv`)\n",
    "- For each solution under `<dataset>/solutions/<solution_name>/` that has `edges.csv`\n",
    "  (or `cost.csv`), runs the **length-only transport simulation** if not already done.\n",
    "- Saves per-solution results under `<solution>/sim_v0/`, and updates a global\n",
    "  comparison table at `<dataset_root>/transport_sim_v0_overview.csv`.\n",
    "\n",
    "Model (high level):\n",
    "- Demand purposes: work, recreation, family, vacation, tourism, emergency\n",
    "- Annual **per-capita intercity trip rates** vary by city size (small/medium/large),\n",
    "  derived from population quantiles (50% / 85% thresholds).\n",
    "- **Destination choice** uses a gravity form:\n",
    "    weight_j ∝ pop_j^α / (euclid_dist_ij + 1)^β_p   (i ≠ j)\n",
    "- Travel time uses **network shortest-path distance** / purpose speed_kmh\n",
    "  (network built from solution edges; no capacity limits ⇒ all trips complete).\n",
    "- Metrics: total trips, total person-hours, avg time (min), avg distance (km),\n",
    "  weighted detour ratio (network_dist / euclid_dist), and a scalar **EfficiencyScore**\n",
    "  = trips_year / person_hours (higher is better, ≈ trips per travel-hour).\n",
    "\n",
    "Edit `SimV0Config.dataset_root` and run.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class SimV0Config:\n",
    "    # Root containing many datasets (each has a top-level nodes.csv)\n",
    "    dataset_root: str = r\"C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\"\n",
    "    # Subfolder that holds solutions in each dataset\n",
    "    solutions_subdir: str = \"solutions\"\n",
    "    # Where to store global comparison CSV under dataset_root\n",
    "    global_summary_csv: str = \"transport_sim_v0_overview.csv\"\n",
    "\n",
    "    # Purpose set\n",
    "    purposes: Tuple[str, ...] = (\"work\", \"recreation\", \"family\", \"vacation\", \"tourism\", \"emergency\")\n",
    "\n",
    "    # Gravity parameters\n",
    "    alpha_pop: float = 1.05  # destination attraction exponent on population\n",
    "    beta_by_purpose: Dict[str, float] = None  # distance-decay exponents\n",
    "\n",
    "    # Speeds by purpose (km/h) — coarse averages\n",
    "    speed_kmh_by_purpose: Dict[str, float] = None\n",
    "\n",
    "    # Annual per-capita intercity trip rates by city \"size\" (small/medium/large)\n",
    "    # These are *expected* intercity trips per person per year for each purpose.\n",
    "    rates_annual_by_class: Dict[str, Dict[str, float]] = None\n",
    "\n",
    "    # Random seed for any tie-breaking\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def _default_params(cfg: SimV0Config) -> None:\n",
    "    if cfg.beta_by_purpose is None:\n",
    "        cfg.beta_by_purpose = {\n",
    "            \"work\": 1.6,\n",
    "            \"recreation\": 1.2,\n",
    "            \"family\": 1.1,\n",
    "            \"vacation\": 0.9,\n",
    "            \"tourism\": 0.8,\n",
    "            \"emergency\": 1.3,\n",
    "        }\n",
    "    if cfg.speed_kmh_by_purpose is None:\n",
    "        cfg.speed_kmh_by_purpose = {\n",
    "            \"work\": 80.0,\n",
    "            \"recreation\": 70.0,\n",
    "            \"family\": 75.0,\n",
    "            \"vacation\": 85.0,\n",
    "            \"tourism\": 75.0,\n",
    "            \"emergency\": 90.0,\n",
    "        }\n",
    "    if cfg.rates_annual_by_class is None:\n",
    "        # Rough intercity frequencies (per person per year)\n",
    "        cfg.rates_annual_by_class = {\n",
    "            \"small\":   {\"work\": 6.0,  \"recreation\": 3.0,  \"family\": 2.0, \"vacation\": 1.2, \"tourism\": 1.5, \"emergency\": 0.05},\n",
    "            \"medium\":  {\"work\": 8.0,  \"recreation\": 3.5,  \"family\": 2.3, \"vacation\": 1.5, \"tourism\": 1.8, \"emergency\": 0.06},\n",
    "            \"large\":   {\"work\": 10.0, \"recreation\": 4.0,  \"family\": 2.6, \"vacation\": 1.8, \"tourism\": 2.0, \"emergency\": 0.07},\n",
    "        }\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# IO helpers\n",
    "# ------------------------------\n",
    "def _iter_datasets_with_nodes(dataset_root: str, solutions_subdir: str) -> Iterable[str]:\n",
    "    for root, dirs, files in os.walk(dataset_root):\n",
    "        # don't walk into nested solutions\n",
    "        dirs[:] = [d for d in dirs if d != solutions_subdir]\n",
    "        if \"nodes.csv\" in files:\n",
    "            yield root\n",
    "\n",
    "\n",
    "def _iter_solutions(dataset_dir: str, solutions_subdir: str) -> Iterable[str]:\n",
    "    sol_root = os.path.join(dataset_dir, solutions_subdir)\n",
    "    if not os.path.isdir(sol_root):\n",
    "        return\n",
    "    for name in sorted(os.listdir(sol_root)):\n",
    "        sdir = os.path.join(sol_root, name)\n",
    "        if os.path.isdir(sdir):\n",
    "            yield sdir\n",
    "\n",
    "\n",
    "def _solution_has_required_files(solution_dir: str) -> bool:\n",
    "    return os.path.exists(os.path.join(solution_dir, \"edges.csv\")) or os.path.exists(os.path.join(solution_dir, \"cost.csv\"))\n",
    "\n",
    "\n",
    "def _simulation_done(solution_dir: str) -> bool:\n",
    "    return os.path.exists(os.path.join(solution_dir, \"sim_v0\", \"summary.json\"))\n",
    "\n",
    "\n",
    "def _load_nodes(nodes_csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(nodes_csv_path)\n",
    "    req = {\"id\", \"x_km\", \"y_km\", \"pop\"}\n",
    "    if not req.issubset(df.columns):\n",
    "        raise ValueError(f\"nodes.csv missing columns: {req - set(df.columns)}\")\n",
    "    df = df.sort_values(\"id\").reset_index(drop=True)\n",
    "    if not np.array_equal(df[\"id\"].to_numpy(), np.arange(len(df))):\n",
    "        # Remap to 0..N-1 if needed\n",
    "        df.insert(0, \"orig_id\", df[\"id\"].values)\n",
    "        df[\"id\"] = np.arange(len(df), dtype=int)\n",
    "    df = df.set_index(\"id\", drop=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _cost_matrix_from_solution(nodes: pd.DataFrame, solution_dir: str) -> np.ndarray:\n",
    "    n = len(nodes)\n",
    "    C = np.full((n, n), np.inf, dtype=float)\n",
    "    np.fill_diagonal(C, 0.0)\n",
    "    edges_csv = os.path.join(solution_dir, \"edges.csv\")\n",
    "    cost_csv  = os.path.join(solution_dir, \"cost.csv\")\n",
    "    if os.path.exists(edges_csv):\n",
    "        e = pd.read_csv(edges_csv)\n",
    "        for _, r in e.iterrows():\n",
    "            u, v, w = int(r[\"u\"]), int(r[\"v\"]), float(r[\"length_km\"])\n",
    "            C[u, v] = C[v, u] = w\n",
    "    elif os.path.exists(cost_csv):\n",
    "        C = pd.read_csv(cost_csv, header=None).to_numpy(dtype=float)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither edges.csv nor cost.csv found.\")\n",
    "    return C\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Math helpers\n",
    "# ------------------------------\n",
    "def _euclidean_matrix(nodes: pd.DataFrame) -> np.ndarray:\n",
    "    x = nodes[\"x_km\"].to_numpy()\n",
    "    y = nodes[\"y_km\"].to_numpy()\n",
    "    X = x[:, None] - x[None, :]\n",
    "    Y = y[:, None] - y[None, :]\n",
    "    return np.sqrt(X * X + Y * Y)\n",
    "\n",
    "\n",
    "def _floyd_warshall(C: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"All-pairs shortest paths for non-negative costs (Naive O(N^3), fine for N<=200).\"\"\"\n",
    "    D = C.copy()\n",
    "    n = D.shape[0]\n",
    "    for k in range(n):\n",
    "        # vectorized relaxation\n",
    "        D = np.minimum(D, D[:, [k]] + D[[k], :])\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Demand model\n",
    "# ------------------------------\n",
    "def _city_classes(pop: np.ndarray) -> List[str]:\n",
    "    q50 = np.percentile(pop, 50)\n",
    "    q85 = np.percentile(pop, 85)\n",
    "    cls = []\n",
    "    for p in pop:\n",
    "        if p < q50:\n",
    "            cls.append(\"small\")\n",
    "        elif p < q85:\n",
    "            cls.append(\"medium\")\n",
    "        else:\n",
    "            cls.append(\"large\")\n",
    "    return cls\n",
    "\n",
    "\n",
    "def _annual_trips_by_origin(nodes: pd.DataFrame, purposes: Tuple[str, ...], rates_by_class: Dict[str, Dict[str, float]]) -> np.ndarray:\n",
    "    \"\"\"Return array R (n, P) of *annual* trips originating at i for each purpose p.\"\"\"\n",
    "    n = len(nodes)\n",
    "    P = len(purposes)\n",
    "    pop = nodes[\"pop\"].to_numpy().astype(float)\n",
    "    classes = _city_classes(pop)\n",
    "    R = np.zeros((n, P), dtype=float)\n",
    "    for i in range(n):\n",
    "        cls = classes[i]\n",
    "        for p_idx, p in enumerate(purposes):\n",
    "            R[i, p_idx] = pop[i] * rates_by_class[cls][p]\n",
    "    return R\n",
    "\n",
    "\n",
    "def _od_share_weights(pop: np.ndarray, D_euclid: np.ndarray,\n",
    "                      alpha: float,\n",
    "                      beta: Dict[str, float],\n",
    "                      purposes: Tuple[str, ...]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each purpose p, compute W_p(i,j) ∝ pop_j^alpha / (dist_ij + 1)^beta_p  for j!=i,\n",
    "    row-normalized over j. Returns W with shape (P, n, n).\n",
    "\n",
    "    Fix: robust handling of rows whose denominator is 0; set uniform over j≠i\n",
    "    and correctly zero the diagonal for only those rows (no shape mismatch).\n",
    "    \"\"\"\n",
    "    n = len(pop)\n",
    "    P = len(purposes)\n",
    "    W = np.zeros((P, n, n), dtype=float)\n",
    "    if n <= 1:\n",
    "        return W  # nothing to distribute\n",
    "\n",
    "    pop_attr = np.power(pop.astype(float), alpha)            # (n,)\n",
    "    M = D_euclid + 1.0                                       # soften small distances\n",
    "\n",
    "    for p_idx, p in enumerate(purposes):\n",
    "        numer = pop_attr[None, :] / np.power(M, beta[p])     # (n, n)\n",
    "        np.fill_diagonal(numer, 0.0)\n",
    "\n",
    "        denom = numer.sum(axis=1, keepdims=True)             # (n, 1)\n",
    "        zero_rows_mask = (denom[:, 0] <= 0.0)\n",
    "\n",
    "        if np.any(zero_rows_mask):\n",
    "            rows = np.where(zero_rows_mask)[0]               # indices of problematic rows\n",
    "            numer[rows, :] = 1.0 / (n - 1)                   # uniform across destinations\n",
    "            numer[rows, rows] = 0.0                          # but not to self\n",
    "            denom = numer.sum(axis=1, keepdims=True)         # recompute; now safe\n",
    "\n",
    "        W[p_idx] = numer / denom\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Simulation core\n",
    "# ------------------------------\n",
    "def simulate_solution(nodes: pd.DataFrame,\n",
    "                      cost_net: np.ndarray,\n",
    "                      purposes: Tuple[str, ...],\n",
    "                      alpha: float,\n",
    "                      beta: Dict[str, float],\n",
    "                      speeds: Dict[str, float],\n",
    "                      rates_by_class: Dict[str, Dict[str, float]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Deterministic expected-value simulation (no capacity limits):\n",
    "    - Build all-pairs shortest path distances\n",
    "    - Annual origin volumes by purpose (pop * rate_class)\n",
    "    - Destination split by gravity weights\n",
    "    - Travel time = distance / speed[purpose]\n",
    "    \"\"\"\n",
    "    n = len(nodes)\n",
    "    P = len(purposes)\n",
    "\n",
    "    # Distances\n",
    "    D_net = _floyd_warshall(cost_net)\n",
    "    D_euclid = _euclidean_matrix(nodes)\n",
    "\n",
    "    # Demand\n",
    "    R = _annual_trips_by_origin(nodes, purposes, rates_by_class)  # (n, P)\n",
    "    W = _od_share_weights(nodes[\"pop\"].to_numpy(), D_euclid, alpha, beta, purposes)  # (P,n,n)\n",
    "\n",
    "    # Annual trips per OD per purpose\n",
    "    # flows[p,i,j] = R[i,p] * W[p,i,j]\n",
    "    flows = np.zeros((P, n, n), dtype=float)\n",
    "    for p_idx in range(P):\n",
    "        flows[p_idx] = R[:, [p_idx]] * W[p_idx]\n",
    "\n",
    "    # Travel time matrix per purpose (hours)\n",
    "    T_hours = np.zeros((P, n, n), dtype=float)\n",
    "    for p_idx, p in enumerate(purposes):\n",
    "        T_hours[p_idx] = D_net / max(1e-6, speeds[p])\n",
    "\n",
    "    # Aggregations\n",
    "    # Exclude i==j from flows & ratios\n",
    "    mask_offdiag = ~np.eye(n, dtype=bool)\n",
    "    total_trips = float(flows[:, mask_offdiag].sum())\n",
    "    total_person_hours = float((flows * T_hours)[:, mask_offdiag].sum())\n",
    "    mean_time_h = total_person_hours / max(1.0, total_trips)\n",
    "    mean_time_min = mean_time_h * 60.0\n",
    "\n",
    "    mean_dist_km = float((flows * D_net)[:, mask_offdiag].sum()) / max(1.0, total_trips)\n",
    "    # Weighted detour ratio vs straight line; ignore pairs with zero euclid\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        detour = D_net / np.where(D_euclid > 0, D_euclid, np.nan)\n",
    "    mean_detour = float((flows * detour)[:, mask_offdiag].sum()) / max(1.0, flows[:, mask_offdiag].sum())\n",
    "\n",
    "    # Purpose breakdown\n",
    "    per_purpose = []\n",
    "    for p_idx, p in enumerate(purposes):\n",
    "        trips_p = float(flows[p_idx, mask_offdiag].sum())\n",
    "        hours_p = float((flows[p_idx] * T_hours[p_idx])[mask_offdiag].sum())\n",
    "        per_purpose.append({\n",
    "            \"purpose\": p,\n",
    "            \"trips\": trips_p,\n",
    "            \"person_hours\": hours_p,\n",
    "            \"avg_time_min\": (hours_p / trips_p * 60.0) if trips_p > 0 else 0.0,\n",
    "            \"avg_dist_km\": float((flows[p_idx] * D_net)[mask_offdiag].sum()) / trips_p if trips_p > 0 else 0.0,\n",
    "        })\n",
    "\n",
    "    # Scalar score: trips per travel-hour (higher is better). Equivalent to 1/mean_time_h.\n",
    "    efficiency_score = total_trips / max(1e-9, total_person_hours)\n",
    "\n",
    "    return {\n",
    "        \"n_nodes\": int(n),\n",
    "        \"total_trips_year\": total_trips,\n",
    "        \"total_person_hours\": total_person_hours,\n",
    "        \"avg_time_min\": mean_time_min,\n",
    "        \"avg_dist_km\": mean_dist_km,\n",
    "        \"avg_detour_ratio\": mean_detour,\n",
    "        \"efficiency_score\": efficiency_score,\n",
    "        \"per_purpose\": per_purpose,\n",
    "        \"purposes\": list(purposes),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Saving + comparison\n",
    "# ------------------------------\n",
    "def _save_solution_results(solution_dir: str, sim: Dict[str, Any], cfg: SimV0Config) -> str:\n",
    "    out_dir = os.path.join(solution_dir, \"sim_v0\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # JSON summary\n",
    "    json_path = os.path.join(out_dir, \"summary.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "            \"config\": {\n",
    "                \"alpha_pop\": cfg.alpha_pop,\n",
    "                \"beta_by_purpose\": cfg.beta_by_purpose,\n",
    "                \"speed_kmh_by_purpose\": cfg.speed_kmh_by_purpose,\n",
    "                \"rates_annual_by_class\": cfg.rates_annual_by_class,\n",
    "            },\n",
    "            \"results\": sim,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # CSV per-purpose\n",
    "    df_pp = pd.DataFrame(sim[\"per_purpose\"])\n",
    "    df_pp.to_csv(os.path.join(out_dir, \"per_purpose.csv\"), index=False)\n",
    "\n",
    "    # Flat summary CSV for quick comparison\n",
    "    flat = {\n",
    "        \"total_trips_year\": sim[\"total_trips_year\"],\n",
    "        \"total_person_hours\": sim[\"total_person_hours\"],\n",
    "        \"avg_time_min\": sim[\"avg_time_min\"],\n",
    "        \"avg_dist_km\": sim[\"avg_dist_km\"],\n",
    "        \"avg_detour_ratio\": sim[\"avg_detour_ratio\"],\n",
    "        \"efficiency_score\": sim[\"efficiency_score\"],\n",
    "        \"n_nodes\": sim[\"n_nodes\"],\n",
    "    }\n",
    "    pd.DataFrame([flat]).to_csv(os.path.join(out_dir, \"summary_flat.csv\"), index=False)\n",
    "    return json_path\n",
    "\n",
    "\n",
    "def _update_global_overview(dataset_root: str,\n",
    "                            dataset_dir: str,\n",
    "                            solution_dir: str,\n",
    "                            sim: Dict[str, Any],\n",
    "                            overview_name: str) -> None:\n",
    "    # derive ids\n",
    "    ds_id = os.path.relpath(dataset_dir, dataset_root).replace(\"\\\\\", \"/\")\n",
    "    sol_id = os.path.basename(solution_dir)\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": ds_id,\n",
    "        \"solution\": sol_id,\n",
    "        \"n_nodes\": sim[\"n_nodes\"],\n",
    "        \"total_trips_year\": sim[\"total_trips_year\"],\n",
    "        \"total_person_hours\": sim[\"total_person_hours\"],\n",
    "        \"avg_time_min\": sim[\"avg_time_min\"],\n",
    "        \"avg_dist_km\": sim[\"avg_dist_km\"],\n",
    "        \"avg_detour_ratio\": sim[\"avg_detour_ratio\"],\n",
    "        \"efficiency_score\": sim[\"efficiency_score\"],\n",
    "    }\n",
    "\n",
    "    overview_path = os.path.join(dataset_root, overview_name)\n",
    "    if os.path.exists(overview_path):\n",
    "        df = pd.read_csv(overview_path)\n",
    "        # upsert\n",
    "        mask = (df[\"dataset\"] == row[\"dataset\"]) & (df[\"solution\"] == row[\"solution\"])\n",
    "        if mask.any():\n",
    "            df.loc[mask, list(row.keys())[2:]] = pd.Series(row)[list(row.keys())[2:]].values\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([row])\n",
    "\n",
    "    df.to_csv(overview_path, index=False)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Orchestration\n",
    "# ------------------------------\n",
    "def main(cfg: SimV0Config) -> None:\n",
    "    _default_params(cfg)\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    datasets = list(_iter_datasets_with_nodes(cfg.dataset_root, cfg.solutions_subdir))\n",
    "    if not datasets:\n",
    "        print(f\"No datasets with nodes.csv found under: {cfg.dataset_root}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Datasets found: {len(datasets)}\")\n",
    "    created = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for ds in datasets:\n",
    "        nodes_csv = os.path.join(ds, \"nodes.csv\")\n",
    "        nodes = _load_nodes(nodes_csv)\n",
    "\n",
    "        for sol in _iter_solutions(ds, cfg.solutions_subdir):\n",
    "            if not _solution_has_required_files(sol):\n",
    "                continue\n",
    "            if _simulation_done(sol):\n",
    "                print(f\"[SKIP] sim_v0 already exists → {os.path.relpath(sol, cfg.dataset_root)}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Build network cost matrix\n",
    "            C = _cost_matrix_from_solution(nodes, sol)\n",
    "\n",
    "            # Run simulation\n",
    "            sim_res = simulate_solution(\n",
    "                nodes=nodes,\n",
    "                cost_net=C,\n",
    "                purposes=cfg.purposes,\n",
    "                alpha=cfg.alpha_pop,\n",
    "                beta=cfg.beta_by_purpose,\n",
    "                speeds=cfg.speed_kmh_by_purpose,\n",
    "                rates_by_class=cfg.rates_annual_by_class,\n",
    "            )\n",
    "\n",
    "            # Save per-solution and update global comparison\n",
    "            _save_solution_results(sol, sim_res, cfg)\n",
    "            _update_global_overview(cfg.dataset_root, ds, sol, sim_res, cfg.global_summary_csv)\n",
    "            print(f\"[OK] sim_v0 created → {os.path.relpath(sol, cfg.dataset_root)}\")\n",
    "            created += 1\n",
    "\n",
    "    print(f\"\\n[Transport Sim V0] Done. New simulations: {created} | Skipped (already existed): {skipped}\")\n",
    "    print(f\"Global overview: {os.path.join(cfg.dataset_root, cfg.global_summary_csv)}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "_cfg = SimV0Config(\n",
    "    dataset_root=r\"C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\",\n",
    "    solutions_subdir=\"solutions\",\n",
    "    global_summary_csv=\"transport_sim_v0_overview.csv\",\n",
    ")\n",
    "main(_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dcf2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset 0 — sv1.2/dv0.1_ds01_seed42_tp3M_nc15\n",
      "---------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.6941, avg_time=86.4 min, avg_dist=110.7 km, detour=1.53, trips=53.74M, hours=77.43M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1506, avg_time=398.3 min, avg_dist=511.2 km, detour=10.68, trips=53.74M, hours=356.81M\n",
      "\n",
      "Dataset 1 — sv1.2/dv0.1_ds02_seed43_tp3.5M_nc18\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.9222, avg_time=65.1 min, avg_dist=83.5 km, detour=1.80, trips=66.73M, hours=72.36M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.2430, avg_time=246.9 min, avg_dist=317.0 km, detour=7.33, trips=66.73M, hours=274.61M\n",
      "\n",
      "Dataset 2 — sv1.2/dv0.1_ds03_seed44_tp4M_nc20\n",
      "---------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.7533, avg_time=79.6 min, avg_dist=102.1 km, detour=1.70, trips=72.60M, hours=96.38M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.3144, avg_time=190.9 min, avg_dist=244.9 km, detour=5.23, trips=72.60M, hours=230.96M\n",
      "\n",
      "Dataset 3 — sv1.2/dv0.1_ds04_seed45_tp2.5M_nc22\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.6953, avg_time=86.3 min, avg_dist=110.6 km, detour=1.74, trips=45.39M, hours=65.28M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1828, avg_time=328.3 min, avg_dist=421.4 km, detour=9.14, trips=45.39M, hours=248.32M\n",
      "\n",
      "Dataset 4 — sv1.2/dv0.1_ds05_seed46_tp5M_nc24\n",
      "---------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.7429, avg_time=80.8 min, avg_dist=103.5 km, detour=1.54, trips=91.04M, hours=122.55M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1992, avg_time=301.1 min, avg_dist=386.7 km, detour=8.47, trips=91.04M, hours=456.93M\n",
      "\n",
      "Dataset 5 — sv1.2/dv0.1_ds06_seed47_tp5.5M_nc26\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.7920, avg_time=75.8 min, avg_dist=97.1 km, detour=1.78, trips=101.66M, hours=128.36M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1370, avg_time=437.9 min, avg_dist=562.3 km, detour=15.83, trips=101.66M, hours=742.01M\n",
      "\n",
      "Dataset 6 — sv1.2/dv0.1_ds07_seed48_tp1M_nc28\n",
      "---------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.8818, avg_time=68.0 min, avg_dist=87.1 km, detour=1.56, trips=18.21M, hours=20.65M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1380, avg_time=434.8 min, avg_dist=558.2 km, detour=17.21, trips=18.21M, hours=131.94M\n",
      "\n",
      "Dataset 7 — sv1.2/dv0.1_ds08_seed49_tp6.5M_nc30\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.5199, avg_time=115.4 min, avg_dist=147.9 km, detour=2.36, trips=115.00M, hours=221.20M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1569, avg_time=382.4 min, avg_dist=490.8 km, detour=10.70, trips=115.00M, hours=732.94M\n",
      "\n",
      "Dataset 8 — sv1.2/dv0.1_ds09_seed50_tp0.5M_nc32\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.7998, avg_time=75.0 min, avg_dist=96.1 km, detour=1.90, trips=8.88M, hours=11.10M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.2002, avg_time=299.7 min, avg_dist=384.6 km, detour=11.49, trips=8.88M, hours=44.34M\n",
      "\n",
      "Dataset 9 — sv1.2/dv0.1_ds10_seed51_tp1.5M_nc35\n",
      "-----------------------------------------------\n",
      "- V1 Greedy MST (v1_greedy_mst): eff=0.5989, avg_time=100.2 min, avg_dist=128.4 km, detour=2.71, trips=27.47M, hours=45.86M\n",
      "- V0 Random Tree (v0_random_tree_seed42): eff=0.1563, avg_time=383.9 min, avg_dist=492.8 km, detour=13.56, trips=27.47M, hours=175.72M\n",
      "\n",
      "Best solution per dataset\n",
      "-------------------------\n",
      "                            dataset      solution          algo  n_nodes  efficiency_score  avg_time_min  avg_dist_km  avg_detour_ratio  total_trips_year  total_person_hours\n",
      "  sv1.2/dv0.1_ds01_seed42_tp3M_nc15 v1_greedy_mst V1 Greedy MST       15          0.694053     86.448708   110.710092          1.534830       53743908.48        7.743486e+07\n",
      "sv1.2/dv0.1_ds02_seed43_tp3.5M_nc18 v1_greedy_mst V1 Greedy MST       18          0.922164     65.064367    83.520383          1.804603       66727001.09        7.235917e+07\n",
      "  sv1.2/dv0.1_ds03_seed44_tp4M_nc20 v1_greedy_mst V1 Greedy MST       20          0.753310     79.648522   102.101331          1.695189       72604710.29        9.638096e+07\n",
      "sv1.2/dv0.1_ds04_seed45_tp2.5M_nc22 v1_greedy_mst V1 Greedy MST       22          0.695300     86.293728   110.577947          1.736642       45388800.07        6.527948e+07\n",
      "  sv1.2/dv0.1_ds05_seed46_tp5M_nc24 v1_greedy_mst V1 Greedy MST       24          0.742880     80.766769   103.476565          1.540267       91040592.88        1.225509e+08\n",
      "sv1.2/dv0.1_ds06_seed47_tp5.5M_nc26 v1_greedy_mst V1 Greedy MST       26          0.792043     75.753435    97.076308          1.782373      101664066.64        1.283567e+08\n",
      "  sv1.2/dv0.1_ds07_seed48_tp1M_nc28 v1_greedy_mst V1 Greedy MST       28          0.881830     68.040354    87.106737          1.561577       18205942.92        2.064565e+07\n",
      "sv1.2/dv0.1_ds08_seed49_tp6.5M_nc30 v1_greedy_mst V1 Greedy MST       30          0.519863    115.415059   147.889651          2.361308      114995636.82        2.212038e+08\n",
      "sv1.2/dv0.1_ds09_seed50_tp0.5M_nc32 v1_greedy_mst V1 Greedy MST       32          0.799750     75.023410    96.100425          1.903556        8877330.38        1.110013e+07\n",
      "sv1.2/dv0.1_ds10_seed51_tp1.5M_nc35 v1_greedy_mst V1 Greedy MST       35          0.598921    100.180165   128.410432          2.707494       27465315.51        4.585800e+07\n",
      "\n",
      "Stats by algorithm\n",
      "------------------\n",
      "                 n   avg_eff   med_eff  avg_time_min  avg_dist_km  avg_detour\n",
      "algo                                                                         \n",
      "V1 Greedy MST   10  0.740011  0.748095     83.263452   106.696987    1.862784\n",
      "V0 Random Tree  10  0.187838  0.169839    340.429040   436.986252   10.964023\n",
      "\n",
      "Saved files:\n",
      " - Grouped markdown: C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\\transport_sim_v0_by_dataset.md\n",
      " - Best by dataset CSV: C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\\transport_sim_v0_best_by_dataset.csv\n",
      " - Stats by algorithm CSV: C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\\transport_sim_v0_stats_by_algo.csv\n"
     ]
    }
   ],
   "source": [
    "# Transport Simulation V0 — Per-dataset grouped report (plus Best-by-dataset & Stats-by-algorithm)\n",
    "# (tabulate-free version)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Config (edit if needed) ---\n",
    "dataset_root = r\"C:\\Users\\User\\Documents\\Code\\traffic-optimization\\00_Datasets\"\n",
    "overview_csv  = os.path.join(dataset_root, \"transport_sim_v0_overview.csv\")\n",
    "grouped_md    = os.path.join(dataset_root, \"transport_sim_v0_by_dataset.md\")\n",
    "best_csv      = os.path.join(dataset_root, \"transport_sim_v0_best_by_dataset.csv\")\n",
    "stats_csv     = os.path.join(dataset_root, \"transport_sim_v0_stats_by_algo.csv\")\n",
    "\n",
    "def _fmt_num(x):\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "    if x >= 1e9:  return f\"{x/1e9:.2f}B\"\n",
    "    if x >= 1e6:  return f\"{x/1e6:.2f}M\"\n",
    "    if x >= 1e3:  return f\"{x/1e3:.2f}k\"\n",
    "    return f\"{x:.0f}\"\n",
    "\n",
    "def _algo_from_solution(name: str) -> str:\n",
    "    n = str(name).lower()\n",
    "    if \"v1\" in n and (\"mst\" in n or \"greedy\" in n): return \"V1 Greedy MST\"\n",
    "    if \"v0\" in n and (\"random\" in n or \"tree\" in n): return \"V0 Random Tree\"\n",
    "    return \"Other\"\n",
    "\n",
    "def _df_to_markdown(df: pd.DataFrame, float_decimals: int = 3) -> str:\n",
    "    \"\"\"Simple, dependency-free DataFrame → Markdown 'pipe' table.\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    header = \"| \" + \" | \".join(cols) + \" |\"\n",
    "    sep    = \"|\" + \"|\".join([\"---\"] * len(cols)) + \"|\"\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        vals = []\n",
    "        for c in cols:\n",
    "            v = r[c]\n",
    "            if isinstance(v, (float, np.floating)):\n",
    "                vals.append(f\"{v:.{float_decimals}f}\")\n",
    "            elif isinstance(v, (int, np.integer)):\n",
    "                vals.append(str(int(v)))\n",
    "            else:\n",
    "                vals.append(str(v))\n",
    "        rows.append(\"| \" + \" | \".join(vals) + \" |\")\n",
    "    return \"\\n\".join([header, sep] + rows)\n",
    "\n",
    "if not os.path.exists(overview_csv):\n",
    "    print(f\"Overview file not found:\\n  {overview_csv}\\nRun the simulation first.\")\n",
    "else:\n",
    "    df = pd.read_csv(overview_csv).copy()\n",
    "    # (Re)derive algo label to be safe\n",
    "    df[\"algo\"] = df[\"solution\"].map(_algo_from_solution)\n",
    "\n",
    "    # ------- Grouped by dataset (printed + markdown) -------\n",
    "    md = [\"# Transport Simulation V0 — Grouped by Dataset\", \"\"]\n",
    "    for ds_idx, (ds_name, g) in enumerate(sorted(df.groupby(\"dataset\"), key=lambda kv: kv[0])):\n",
    "        g = g.sort_values(\"efficiency_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Print block\n",
    "        header = f\"Dataset {ds_idx} — {ds_name}\"\n",
    "        print(\"\\n\" + header)\n",
    "        print(\"-\" * len(header))\n",
    "        for _, r in g.iterrows():\n",
    "            algo = _algo_from_solution(r[\"solution\"])\n",
    "            line = (\n",
    "                f\"- {algo} ({r['solution']}): \"\n",
    "                f\"eff={r['efficiency_score']:.4f}, \"\n",
    "                f\"avg_time={r['avg_time_min']:.1f} min, \"\n",
    "                f\"avg_dist={r['avg_dist_km']:.1f} km, \"\n",
    "                f\"detour={r['avg_detour_ratio']:.2f}, \"\n",
    "                f\"trips={_fmt_num(r['total_trips_year'])}, \"\n",
    "                f\"hours={_fmt_num(r['total_person_hours'])}\"\n",
    "            )\n",
    "            print(line)\n",
    "\n",
    "        # Markdown block\n",
    "        md.append(f\"## Dataset {ds_idx} — `{ds_name}`\")\n",
    "        for _, r in g.iterrows():\n",
    "            algo = _algo_from_solution(r[\"solution\"])\n",
    "            md.append(\n",
    "                f\"- **{algo}** (`{r['solution']}`): \"\n",
    "                f\"eff={r['efficiency_score']:.4f}, \"\n",
    "                f\"avg_time={r['avg_time_min']:.1f} min, \"\n",
    "                f\"avg_dist={r['avg_dist_km']:.1f} km, \"\n",
    "                f\"detour={r['avg_detour_ratio']:.2f}, \"\n",
    "                f\"trips={_fmt_num(r['total_trips_year'])}, \"\n",
    "                f\"hours={_fmt_num(r['total_person_hours'])}\"\n",
    "            )\n",
    "        md.append(\"\")\n",
    "\n",
    "    # ------- Best solution by dataset -------\n",
    "    idx = df.groupby(\"dataset\")[\"efficiency_score\"].idxmax()\n",
    "    best = df.loc[idx].sort_values(\"dataset\").reset_index(drop=True)\n",
    "    best = best[[\n",
    "        \"dataset\",\"solution\",\"algo\",\"n_nodes\",\n",
    "        \"efficiency_score\",\"avg_time_min\",\"avg_dist_km\",\"avg_detour_ratio\",\n",
    "        \"total_trips_year\",\"total_person_hours\"\n",
    "    ]]\n",
    "\n",
    "    print(\"\\nBest solution per dataset\")\n",
    "    print(\"-------------------------\")\n",
    "    print(best.to_string(index=False))\n",
    "\n",
    "    md += [\n",
    "        \"---\",\n",
    "        \"## Best Solution per Dataset\",\n",
    "        \"\",\n",
    "        \"| Dataset | Solution | Algo | Nodes | Eff. score | Avg time (min) | Avg dist (km) | Detour | Trips | Hours |\",\n",
    "        \"|---|---|---:|---:|---:|---:|---:|---:|---:|---:|\",\n",
    "    ]\n",
    "    for _, r in best.iterrows():\n",
    "        md.append(\n",
    "            f\"| `{r['dataset']}` | `{r['solution']}` | {r['algo']} | {int(r['n_nodes'])} | \"\n",
    "            f\"{r['efficiency_score']:.4f} | {r['avg_time_min']:.1f} | {r['avg_dist_km']:.1f} | \"\n",
    "            f\"{r['avg_detour_ratio']:.2f} | {_fmt_num(r['total_trips_year'])} | {_fmt_num(r['total_person_hours'])} |\"\n",
    "        )\n",
    "    md.append(\"\")\n",
    "\n",
    "    # Save CSV\n",
    "    best.to_csv(best_csv, index=False)\n",
    "\n",
    "    # ------- Stats by algorithm -------\n",
    "    stats = (\n",
    "        df.groupby(\"algo\")\n",
    "          .agg(\n",
    "              n=(\"solution\",\"count\"),\n",
    "              avg_eff=(\"efficiency_score\",\"mean\"),\n",
    "              med_eff=(\"efficiency_score\",\"median\"),\n",
    "              avg_time_min=(\"avg_time_min\",\"mean\"),\n",
    "              avg_dist_km=(\"avg_dist_km\",\"mean\"),\n",
    "              avg_detour=(\"avg_detour_ratio\",\"mean\"),\n",
    "          )\n",
    "          .sort_values(\"avg_eff\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nStats by algorithm\")\n",
    "    print(\"------------------\")\n",
    "    print(stats.to_string())\n",
    "\n",
    "    md += [\n",
    "        \"\",\n",
    "        \"## Stats by Algorithm\",\n",
    "        \"\",\n",
    "        _df_to_markdown(stats.reset_index(), float_decimals=3),\n",
    "        \"\"\n",
    "    ]\n",
    "    stats.to_csv(stats_csv)\n",
    "\n",
    "    # ------- Write markdown file -------\n",
    "    with open(grouped_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(md))\n",
    "\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(f\" - Grouped markdown: {grouped_md}\")\n",
    "    print(f\" - Best by dataset CSV: {best_csv}\")\n",
    "    print(f\" - Stats by algorithm CSV: {stats_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
